{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------\n",
      "keras-unet init: TF version is >= 2.0.0 - using `tf.keras` instead of `Keras`\n",
      "-----------------------------------------\n",
      "Segmentation Models: using `tf.keras` framework.\n",
      "There is a gpu avaliable\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 1;\n",
       "                var nbb_unformatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%matplotlib inline\\n%load_ext tensorboard\\n\\nimport os\\n\\n# os.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"-1\\\" #disable gpu\\n\\n\\nfrom keras_unet.utils import plot_imgs\\nfrom tensorflow import keras\\nfrom tensorflow.keras.callbacks import (\\n    EarlyStopping,\\n    ModelCheckpoint,\\n    ReduceLROnPlateau,\\n    LearningRateScheduler,\\n    TensorBoard,\\n)\\nfrom tensorflow.keras.layers import Multiply, Input\\nfrom tensorflow.keras.models import Model\\nimport tensorflow as tf\\n\\nimport numpy as np\\nfrom functools import partial\\nimport segmentation_models as sm\\nfrom tensorflow.keras.optimizers import Adam, SGD\\n\\nphysical_devices = tf.config.experimental.list_physical_devices(\\\"GPU\\\")\\nif physical_devices:\\n    print(\\\"There is a gpu avaliable\\\")\\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\\n\\nfrom myutils.checkpoints import PlotLearning, PredictAndSave\\nfrom myutils.models import get_se_model, unet\\nfrom myutils.training import (\\n    get_augmented_data,\\n    get_val_generator,\\n    train_val_test_split,\\n    patch_image,\\n    get_predictions_from_patches,\\n    tversky_loss,\\n)\\nfrom myutils.utils import (\\n    download_dataset,\\n    load_images,\\n    process_images,\\n    prepare_for_pyplot,\\n)\\nfrom myutils.datasets import PatchedDataset, PatchedSequence, Dataset\\nfrom myutils.metrics import (\\n    prediction_report,\\n    sensitivity_metric,\\n    specificity_metric,\\n    g_mean_metric,\\n)\\n\\nfrom myutils.focal_tversky_loss import focal_tversky\\n\\nfrom skimage.morphology import dilation, binary_erosion, disk, diamond, area_opening\";\n",
       "                var nbb_formatted_code = \"%load_ext autoreload\\n%autoreload 2\\n%load_ext nb_black\\n%matplotlib inline\\n%load_ext tensorboard\\n\\nimport os\\n\\n# os.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"-1\\\" #disable gpu\\n\\n\\nfrom keras_unet.utils import plot_imgs\\nfrom tensorflow import keras\\nfrom tensorflow.keras.callbacks import (\\n    EarlyStopping,\\n    ModelCheckpoint,\\n    ReduceLROnPlateau,\\n    LearningRateScheduler,\\n    TensorBoard,\\n)\\nfrom tensorflow.keras.layers import Multiply, Input\\nfrom tensorflow.keras.models import Model\\nimport tensorflow as tf\\n\\nimport numpy as np\\nfrom functools import partial\\nimport segmentation_models as sm\\nfrom tensorflow.keras.optimizers import Adam, SGD\\n\\nphysical_devices = tf.config.experimental.list_physical_devices(\\\"GPU\\\")\\nif physical_devices:\\n    print(\\\"There is a gpu avaliable\\\")\\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)\\n\\nfrom myutils.checkpoints import PlotLearning, PredictAndSave\\nfrom myutils.models import get_se_model, unet\\nfrom myutils.training import (\\n    get_augmented_data,\\n    get_val_generator,\\n    train_val_test_split,\\n    patch_image,\\n    get_predictions_from_patches,\\n    tversky_loss,\\n)\\nfrom myutils.utils import (\\n    download_dataset,\\n    load_images,\\n    process_images,\\n    prepare_for_pyplot,\\n)\\nfrom myutils.datasets import PatchedDataset, PatchedSequence, Dataset\\nfrom myutils.metrics import (\\n    prediction_report,\\n    sensitivity_metric,\\n    specificity_metric,\\n    g_mean_metric,\\n)\\n\\nfrom myutils.focal_tversky_loss import focal_tversky\\n\\nfrom skimage.morphology import dilation, binary_erosion, disk, diamond, area_opening\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%load_ext nb_black\n",
    "%matplotlib inline\n",
    "%load_ext tensorboard\n",
    "\n",
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\" #disable gpu\n",
    "\n",
    "\n",
    "from keras_unet.utils import plot_imgs\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    "    ReduceLROnPlateau,\n",
    "    LearningRateScheduler,\n",
    "    TensorBoard,\n",
    ")\n",
    "from tensorflow.keras.layers import Multiply, Input\n",
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import segmentation_models as sm\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "\n",
    "physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "if physical_devices:\n",
    "    print(\"There is a gpu avaliable\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "from myutils.checkpoints import PlotLearning, PredictAndSave\n",
    "from myutils.models import get_se_model, unet\n",
    "from myutils.training import (\n",
    "    get_augmented_data,\n",
    "    get_val_generator,\n",
    "    train_val_test_split,\n",
    "    patch_image,\n",
    "    get_predictions_from_patches,\n",
    "    tversky_loss,\n",
    ")\n",
    "from myutils.utils import (\n",
    "    download_dataset,\n",
    "    load_images,\n",
    "    process_images,\n",
    "    prepare_for_pyplot,\n",
    ")\n",
    "from myutils.datasets import PatchedDataset, PatchedSequence, Dataset\n",
    "from myutils.metrics import (\n",
    "    prediction_report,\n",
    "    sensitivity_metric,\n",
    "    specificity_metric,\n",
    "    g_mean_metric,\n",
    ")\n",
    "\n",
    "from myutils.focal_tversky_loss import focal_tversky\n",
    "\n",
    "from skimage.morphology import dilation, binary_erosion, disk, diamond, area_opening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 45\n",
      "01_dr.JPG            | 01_dr.tif            | 01_dr_mask.tif\n",
      "01_g.jpg             | 01_g.tif             | 01_g_mask.tif\n",
      "01_h.jpg             | 01_h.tif             | 01_h_mask.tif\n",
      "02_dr.JPG            | 02_dr.tif            | 02_dr_mask.tif\n",
      "02_g.jpg             | 02_g.tif             | 02_g_mask.tif\n",
      "02_h.jpg             | 02_h.tif             | 02_h_mask.tif\n",
      "03_dr.JPG            | 03_dr.tif            | 03_dr_mask.tif\n",
      "03_g.jpg             | 03_g.tif             | 03_g_mask.tif\n",
      "03_h.jpg             | 03_h.tif             | 03_h_mask.tif\n",
      "04_dr.JPG            | 04_dr.tif            | 04_dr_mask.tif\n",
      "...\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 2;\n",
       "                var nbb_unformatted_code = \"seed = 2137\\n\\ndata_dir = '/home/maciej/.keras/datasets/'\\ninput_dir = os.path.join(data_dir, 'images')\\ntarget_dir = os.path.join(data_dir, 'manual1')\\nfov_dir = os.path.join(data_dir, 'mask')\\n\\ninput_img_paths = sorted(\\n    [\\n        os.path.join(input_dir, fname)\\n        for fname in os.listdir(input_dir)\\n    ]\\n)\\ntarget_img_paths = sorted(\\n    [\\n        os.path.join(target_dir, fname)\\n        for fname in os.listdir(target_dir)\\n    ]\\n)\\nfov_img_paths = sorted(\\n    [\\n        os.path.join(fov_dir, fname)\\n        for fname in os.listdir(fov_dir)\\n    ]\\n)\\n\\nprint(\\\"Number of samples:\\\", len(input_img_paths))\\n\\nfor input_path, target_path, fov_path in zip(input_img_paths[:10], target_img_paths[:10], fov_img_paths[:10]):\\n    print(input_path.split('/')[-1].ljust(20), \\\"|\\\", target_path.split('/')[-1].ljust(20), \\\"|\\\", fov_path.split('/')[-1])\\nprint(\\\"...\\\")\";\n",
       "                var nbb_formatted_code = \"seed = 2137\\n\\ndata_dir = \\\"/home/maciej/.keras/datasets/\\\"\\ninput_dir = os.path.join(data_dir, \\\"images\\\")\\ntarget_dir = os.path.join(data_dir, \\\"manual1\\\")\\nfov_dir = os.path.join(data_dir, \\\"mask\\\")\\n\\ninput_img_paths = sorted(\\n    [os.path.join(input_dir, fname) for fname in os.listdir(input_dir)]\\n)\\ntarget_img_paths = sorted(\\n    [os.path.join(target_dir, fname) for fname in os.listdir(target_dir)]\\n)\\nfov_img_paths = sorted([os.path.join(fov_dir, fname) for fname in os.listdir(fov_dir)])\\n\\nprint(\\\"Number of samples:\\\", len(input_img_paths))\\n\\nfor input_path, target_path, fov_path in zip(\\n    input_img_paths[:10], target_img_paths[:10], fov_img_paths[:10]\\n):\\n    print(\\n        input_path.split(\\\"/\\\")[-1].ljust(20),\\n        \\\"|\\\",\\n        target_path.split(\\\"/\\\")[-1].ljust(20),\\n        \\\"|\\\",\\n        fov_path.split(\\\"/\\\")[-1],\\n    )\\nprint(\\\"...\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "seed = 2137\n",
    "\n",
    "data_dir = '/home/maciej/.keras/datasets/'\n",
    "input_dir = os.path.join(data_dir, 'images')\n",
    "target_dir = os.path.join(data_dir, 'manual1')\n",
    "fov_dir = os.path.join(data_dir, 'mask')\n",
    "\n",
    "input_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(input_dir, fname)\n",
    "        for fname in os.listdir(input_dir)\n",
    "    ]\n",
    ")\n",
    "target_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(target_dir, fname)\n",
    "        for fname in os.listdir(target_dir)\n",
    "    ]\n",
    ")\n",
    "fov_img_paths = sorted(\n",
    "    [\n",
    "        os.path.join(fov_dir, fname)\n",
    "        for fname in os.listdir(fov_dir)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Number of samples:\", len(input_img_paths))\n",
    "\n",
    "for input_path, target_path, fov_path in zip(input_img_paths[:10], target_img_paths[:10], fov_img_paths[:10]):\n",
    "    print(input_path.split('/')[-1].ljust(20), \"|\", target_path.split('/')[-1].ljust(20), \"|\", fov_path.split('/')[-1])\n",
    "print(\"...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(x_train)=33\n",
      "len(x_val)=6\n",
      "len(x_test)=6\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 3;\n",
       "                var nbb_unformatted_code = \"(\\n    x_train_files,\\n    x_val_files,\\n    x_test_files,\\n    y_train_files,\\n    y_val_files,\\n    y_test_files,\\n    fov_train_files,\\n    fov_val_files,\\n    fov_test_files,\\n) = train_val_test_split(\\n    input_img_paths,\\n    target_img_paths,\\n    fov_img_paths,\\n    random_state=seed,\\n    train_size=0.75,\\n    test_size=0.5,\\n)\";\n",
       "                var nbb_formatted_code = \"(\\n    x_train_files,\\n    x_val_files,\\n    x_test_files,\\n    y_train_files,\\n    y_val_files,\\n    y_test_files,\\n    fov_train_files,\\n    fov_val_files,\\n    fov_test_files,\\n) = train_val_test_split(\\n    input_img_paths,\\n    target_img_paths,\\n    fov_img_paths,\\n    random_state=seed,\\n    train_size=0.75,\\n    test_size=0.5,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "(\n",
    "    x_train_files,\n",
    "    x_val_files,\n",
    "    x_test_files,\n",
    "    y_train_files,\n",
    "    y_val_files,\n",
    "    y_test_files,\n",
    "    fov_train_files,\n",
    "    fov_val_files,\n",
    "    fov_test_files,\n",
    ") = train_val_test_split(\n",
    "    input_img_paths,\n",
    "    target_img_paths,\n",
    "    fov_img_paths,\n",
    "    random_state=seed,\n",
    "    train_size=0.75,\n",
    "    test_size=0.5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 4;\n",
       "                var nbb_unformatted_code = \"patch_size = 128 * 5\\nbatch_size = 1\\n\\n# dataset = PatchedDataset(x_val_directory, y_val_directory, patch_size, patch_size, f'val_{patch_size=}stride={patch_size}')\\ntrain_dataset = Dataset(\\\"train\\\").from_files(\\n    x_train_files,\\n    y_train_files,\\n    fov_train_files,\\n    patch_size=patch_size,\\n    stride=patch_size,\\n)\\nval_dataset = Dataset(\\\"val\\\").from_files(\\n    x_val_files, y_val_files, fov_val_files, patch_size=patch_size, stride=patch_size\\n)\\ntest_dataset = Dataset(\\\"test\\\").from_files(x_test_files, y_test_files, fov_test_files)\";\n",
       "                var nbb_formatted_code = \"patch_size = 128 * 5\\nbatch_size = 1\\n\\n# dataset = PatchedDataset(x_val_directory, y_val_directory, patch_size, patch_size, f'val_{patch_size=}stride={patch_size}')\\ntrain_dataset = Dataset(\\\"train\\\").from_files(\\n    x_train_files,\\n    y_train_files,\\n    fov_train_files,\\n    patch_size=patch_size,\\n    stride=patch_size,\\n)\\nval_dataset = Dataset(\\\"val\\\").from_files(\\n    x_val_files, y_val_files, fov_val_files, patch_size=patch_size, stride=patch_size\\n)\\ntest_dataset = Dataset(\\\"test\\\").from_files(x_test_files, y_test_files, fov_test_files)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "patch_size = 128 * 5\n",
    "batch_size = 1\n",
    "\n",
    "# dataset = PatchedDataset(x_val_directory, y_val_directory, patch_size, patch_size, f'val_{patch_size=}stride={patch_size}')\n",
    "train_dataset = Dataset(\"train\").from_files(\n",
    "    x_train_files,\n",
    "    y_train_files,\n",
    "    fov_train_files,\n",
    "    patch_size=patch_size,\n",
    "    stride=patch_size,\n",
    ")\n",
    "val_dataset = Dataset(\"val\").from_files(\n",
    "    x_val_files, y_val_files, fov_val_files, patch_size=patch_size, stride=patch_size\n",
    ")\n",
    "test_dataset = Dataset(\"test\").from_files(x_test_files, y_test_files, fov_test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 5;\n",
       "                var nbb_unformatted_code = \"from tensorflow.keras.utils import Sequence\\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\\nimport albumentations as A\\nimport math\\n\\n\\nclass AugmentedSequence(Sequence):\\n    def __init__(\\n        self,\\n        x_set,\\n        y_set,\\n        mask_set,\\n        batch_size,\\n        patch_size,\\n        random_seed=None,\\n        augment=True,\\n        cycles=1,\\n    ):\\n        self.x, self.y, self.m = x_set * cycles, y_set * cycles, mask_set * cycles\\n        self.augment = augment\\n        self.seed = seed\\n        self.cycles = cycles\\n        self.batch_size = batch_size\\n        self.patch_size = patch_size\\n        if patch_size is not None:\\n            self.transform = A.Compose(\\n                [\\n                    A.RandomSizedCrop(\\n                        min_max_height=(patch_size // 2, patch_size * 2),\\n                        width=patch_size,\\n                        height=patch_size,\\n                    ),\\n                    # A.RandomCrop(width=patch_size, height=patch_size),\\n                    A.RandomBrightnessContrast(p=0.2),\\n                    A.VerticalFlip(p=0.5),\\n                    A.RandomRotate90(p=0.5),\\n                    A.OneOf(\\n                        [\\n                            A.ElasticTransform(\\n                                p=0.5,\\n                                alpha=120,\\n                                sigma=120 * 0.05,\\n                                alpha_affine=120 * 0.03,\\n                            ),\\n                            A.GridDistortion(p=0.5),\\n                            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\\n                        ],\\n                        p=0.8,\\n                    ),\\n                ],\\n                additional_targets={\\\"mask0\\\": \\\"mask\\\"},\\n            )\\n        else:\\n            self.transform = A.Compose(\\n                [\\n                    A.RandomBrightnessContrast(p=0.2),\\n                    A.VerticalFlip(p=0.5),\\n                    A.RandomRotate90(p=0.5),\\n                    A.OneOf(\\n                        [\\n                            A.ElasticTransform(\\n                                p=0.5,\\n                                alpha=120,\\n                                sigma=120 * 0.05,\\n                                alpha_affine=120 * 0.03,\\n                            ),\\n                            A.GridDistortion(p=0.5),\\n                            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\\n                        ],\\n                        p=0.1,  # 0.8\\n                    ),\\n                ],\\n                additional_targets={\\\"mask0\\\": \\\"mask\\\"},\\n            )\\n\\n        # https://albumentations.ai/docs/examples/example_kaggle_salt/#lets-add-non-rigid-transformations-and-randomsizedcrop\\n\\n    def __len__(self):\\n        return math.ceil(len(self.x) / self.batch_size)\\n\\n    def __getitem__(self, idx):\\n        batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size]\\n        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\\n        batch_m = self.m[idx * self.batch_size : (idx + 1) * self.batch_size]\\n        X, Y, M = [], [], []\\n        for x_path, y_path, m_path in zip(batch_x, batch_y, batch_m):\\n            x, y, m = (\\n                np.array(load_img(x_path)),\\n                np.array(load_img(y_path, color_mode=\\\"grayscale\\\")),\\n                np.array(load_img(m_path, color_mode=\\\"grayscale\\\")),\\n            )\\n            x = x.astype(\\\"float32\\\") / 255\\n            y = np.clip(y.astype(\\\"float32\\\"), 0, 1)\\n            m = np.clip(m.astype(\\\"float32\\\"), 0, 1)\\n            X.append(x)\\n            Y.append(y)\\n            M.append(m)\\n\\n        if self.augment:\\n            X_trans, Y_trans, M_trans = [], [], []\\n            for x, y, m in zip(X, Y, M):\\n                transformed = self.transform(image=x, mask=y, mask0=m)\\n                X_trans.append(transformed[\\\"image\\\"])\\n                Y_trans.append(transformed[\\\"mask\\\"])\\n                M_trans.append(transformed[\\\"mask0\\\"])\\n        else:\\n            X_trans, Y_trans, M_trans = X, Y, M\\n\\n        X_trans, Y_trans, M_trans = (\\n            np.stack(X_trans, axis=0),\\n            np.stack(Y_trans, axis=0),\\n            np.stack(M_trans, axis=0),\\n        )\\n        if Y_trans.ndim != 4:\\n            Y_trans = np.expand_dims(Y_trans, -1)\\n        if M_trans.ndim != 4:\\n            M_trans = np.expand_dims(M_trans, -1)\\n        return (X_trans, M_trans), Y_trans\\n\\n\\nepochs = 500\\n\\ntrain_gen = AugmentedSequence(\\n    x_train_files,\\n    y_train_files,\\n    fov_train_files,\\n    batch_size,\\n    patch_size,\\n    seed,\\n    cycles=1,\\n)\\n\\\"\\\"\\\"\\ntrain_gen = AugmentedSequence(\\n    train_dataset.x_paths,\\n    train_dataset.y_paths,\\n    train_dataset.m_paths,\\n    batch_size,\\n    patch_size,\\n    seed,\\n    augment=False,\\n    cycles=1,\\n)\\n\\\"\\\"\\\"\\n# train_gen = PatchedSequence(x_train_files, y_train_files, batch_size, patch_size, seed)\\n\\nval_gen = AugmentedSequence(\\n    val_dataset.x_paths,\\n    val_dataset.y_paths,\\n    val_dataset.m_paths,\\n    batch_size,\\n    patch_size,\\n    seed,\\n    False,\\n)\\ntest_gen = AugmentedSequence(\\n    test_dataset.x_paths,\\n    test_dataset.y_paths,\\n    test_dataset.m_paths,\\n    1,\\n    None,\\n    seed,\\n    False,\\n)\";\n",
       "                var nbb_formatted_code = \"from tensorflow.keras.utils import Sequence\\nfrom tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\\nimport albumentations as A\\nimport math\\n\\n\\nclass AugmentedSequence(Sequence):\\n    def __init__(\\n        self,\\n        x_set,\\n        y_set,\\n        mask_set,\\n        batch_size,\\n        patch_size,\\n        random_seed=None,\\n        augment=True,\\n        cycles=1,\\n    ):\\n        self.x, self.y, self.m = x_set * cycles, y_set * cycles, mask_set * cycles\\n        self.augment = augment\\n        self.seed = seed\\n        self.cycles = cycles\\n        self.batch_size = batch_size\\n        self.patch_size = patch_size\\n        if patch_size is not None:\\n            self.transform = A.Compose(\\n                [\\n                    A.RandomSizedCrop(\\n                        min_max_height=(patch_size // 2, patch_size * 2),\\n                        width=patch_size,\\n                        height=patch_size,\\n                    ),\\n                    # A.RandomCrop(width=patch_size, height=patch_size),\\n                    A.RandomBrightnessContrast(p=0.2),\\n                    A.VerticalFlip(p=0.5),\\n                    A.RandomRotate90(p=0.5),\\n                    A.OneOf(\\n                        [\\n                            A.ElasticTransform(\\n                                p=0.5,\\n                                alpha=120,\\n                                sigma=120 * 0.05,\\n                                alpha_affine=120 * 0.03,\\n                            ),\\n                            A.GridDistortion(p=0.5),\\n                            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\\n                        ],\\n                        p=0.8,\\n                    ),\\n                ],\\n                additional_targets={\\\"mask0\\\": \\\"mask\\\"},\\n            )\\n        else:\\n            self.transform = A.Compose(\\n                [\\n                    A.RandomBrightnessContrast(p=0.2),\\n                    A.VerticalFlip(p=0.5),\\n                    A.RandomRotate90(p=0.5),\\n                    A.OneOf(\\n                        [\\n                            A.ElasticTransform(\\n                                p=0.5,\\n                                alpha=120,\\n                                sigma=120 * 0.05,\\n                                alpha_affine=120 * 0.03,\\n                            ),\\n                            A.GridDistortion(p=0.5),\\n                            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\\n                        ],\\n                        p=0.1,  # 0.8\\n                    ),\\n                ],\\n                additional_targets={\\\"mask0\\\": \\\"mask\\\"},\\n            )\\n\\n        # https://albumentations.ai/docs/examples/example_kaggle_salt/#lets-add-non-rigid-transformations-and-randomsizedcrop\\n\\n    def __len__(self):\\n        return math.ceil(len(self.x) / self.batch_size)\\n\\n    def __getitem__(self, idx):\\n        batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size]\\n        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\\n        batch_m = self.m[idx * self.batch_size : (idx + 1) * self.batch_size]\\n        X, Y, M = [], [], []\\n        for x_path, y_path, m_path in zip(batch_x, batch_y, batch_m):\\n            x, y, m = (\\n                np.array(load_img(x_path)),\\n                np.array(load_img(y_path, color_mode=\\\"grayscale\\\")),\\n                np.array(load_img(m_path, color_mode=\\\"grayscale\\\")),\\n            )\\n            x = x.astype(\\\"float32\\\") / 255\\n            y = np.clip(y.astype(\\\"float32\\\"), 0, 1)\\n            m = np.clip(m.astype(\\\"float32\\\"), 0, 1)\\n            X.append(x)\\n            Y.append(y)\\n            M.append(m)\\n\\n        if self.augment:\\n            X_trans, Y_trans, M_trans = [], [], []\\n            for x, y, m in zip(X, Y, M):\\n                transformed = self.transform(image=x, mask=y, mask0=m)\\n                X_trans.append(transformed[\\\"image\\\"])\\n                Y_trans.append(transformed[\\\"mask\\\"])\\n                M_trans.append(transformed[\\\"mask0\\\"])\\n        else:\\n            X_trans, Y_trans, M_trans = X, Y, M\\n\\n        X_trans, Y_trans, M_trans = (\\n            np.stack(X_trans, axis=0),\\n            np.stack(Y_trans, axis=0),\\n            np.stack(M_trans, axis=0),\\n        )\\n        if Y_trans.ndim != 4:\\n            Y_trans = np.expand_dims(Y_trans, -1)\\n        if M_trans.ndim != 4:\\n            M_trans = np.expand_dims(M_trans, -1)\\n        return (X_trans, M_trans), Y_trans\\n\\n\\nepochs = 500\\n\\ntrain_gen = AugmentedSequence(\\n    x_train_files,\\n    y_train_files,\\n    fov_train_files,\\n    batch_size,\\n    patch_size,\\n    seed,\\n    cycles=1,\\n)\\n\\\"\\\"\\\"\\ntrain_gen = AugmentedSequence(\\n    train_dataset.x_paths,\\n    train_dataset.y_paths,\\n    train_dataset.m_paths,\\n    batch_size,\\n    patch_size,\\n    seed,\\n    augment=False,\\n    cycles=1,\\n)\\n\\\"\\\"\\\"\\n# train_gen = PatchedSequence(x_train_files, y_train_files, batch_size, patch_size, seed)\\n\\nval_gen = AugmentedSequence(\\n    val_dataset.x_paths,\\n    val_dataset.y_paths,\\n    val_dataset.m_paths,\\n    batch_size,\\n    patch_size,\\n    seed,\\n    False,\\n)\\ntest_gen = AugmentedSequence(\\n    test_dataset.x_paths,\\n    test_dataset.y_paths,\\n    test_dataset.m_paths,\\n    1,\\n    None,\\n    seed,\\n    False,\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.preprocessing.image import load_img, ImageDataGenerator\n",
    "import albumentations as A\n",
    "import math\n",
    "\n",
    "\n",
    "class AugmentedSequence(Sequence):\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_set,\n",
    "        y_set,\n",
    "        mask_set,\n",
    "        batch_size,\n",
    "        patch_size,\n",
    "        random_seed=None,\n",
    "        augment=True,\n",
    "        cycles=1,\n",
    "    ):\n",
    "        self.x, self.y, self.m = x_set * cycles, y_set * cycles, mask_set * cycles\n",
    "        self.augment = augment\n",
    "        self.seed = seed\n",
    "        self.cycles = cycles\n",
    "        self.batch_size = batch_size\n",
    "        self.patch_size = patch_size\n",
    "        if patch_size is not None:\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.RandomSizedCrop(\n",
    "                        min_max_height=(patch_size // 2, patch_size * 2),\n",
    "                        width=patch_size,\n",
    "                        height=patch_size,\n",
    "                    ),\n",
    "                    # A.RandomCrop(width=patch_size, height=patch_size),\n",
    "                    A.RandomBrightnessContrast(p=0.2),\n",
    "                    A.VerticalFlip(p=0.5),\n",
    "                    A.RandomRotate90(p=0.5),\n",
    "                    A.OneOf(\n",
    "                        [\n",
    "                            A.ElasticTransform(\n",
    "                                p=0.5,\n",
    "                                alpha=120,\n",
    "                                sigma=120 * 0.05,\n",
    "                                alpha_affine=120 * 0.03,\n",
    "                            ),\n",
    "                            A.GridDistortion(p=0.5),\n",
    "                            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "                        ],\n",
    "                        p=0.8,\n",
    "                    ),\n",
    "                ],\n",
    "                additional_targets={\"mask0\": \"mask\"},\n",
    "            )\n",
    "        else:\n",
    "            self.transform = A.Compose(\n",
    "                [\n",
    "                    A.RandomBrightnessContrast(p=0.2),\n",
    "                    A.VerticalFlip(p=0.5),\n",
    "                    A.RandomRotate90(p=0.5),\n",
    "                    A.OneOf(\n",
    "                        [\n",
    "                            A.ElasticTransform(\n",
    "                                p=0.5,\n",
    "                                alpha=120,\n",
    "                                sigma=120 * 0.05,\n",
    "                                alpha_affine=120 * 0.03,\n",
    "                            ),\n",
    "                            A.GridDistortion(p=0.5),\n",
    "                            A.OpticalDistortion(distort_limit=1, shift_limit=0.5, p=1),\n",
    "                        ],\n",
    "                        p=0.1,  # 0.8\n",
    "                    ),\n",
    "                ],\n",
    "                additional_targets={\"mask0\": \"mask\"},\n",
    "            )\n",
    "\n",
    "        # https://albumentations.ai/docs/examples/example_kaggle_salt/#lets-add-non-rigid-transformations-and-randomsizedcrop\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.x) / self.batch_size)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_x = self.x[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_y = self.y[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        batch_m = self.m[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        X, Y, M = [], [], []\n",
    "        for x_path, y_path, m_path in zip(batch_x, batch_y, batch_m):\n",
    "            x, y, m = (\n",
    "                np.array(load_img(x_path)),\n",
    "                np.array(load_img(y_path, color_mode=\"grayscale\")),\n",
    "                np.array(load_img(m_path, color_mode=\"grayscale\")),\n",
    "            )\n",
    "            x = x.astype(\"float32\") / 255\n",
    "            y = np.clip(y.astype(\"float32\"), 0, 1)\n",
    "            m = np.clip(m.astype(\"float32\"), 0, 1)\n",
    "            X.append(x)\n",
    "            Y.append(y)\n",
    "            M.append(m)\n",
    "\n",
    "        if self.augment:\n",
    "            X_trans, Y_trans, M_trans = [], [], []\n",
    "            for x, y, m in zip(X, Y, M):\n",
    "                transformed = self.transform(image=x, mask=y, mask0=m)\n",
    "                X_trans.append(transformed[\"image\"])\n",
    "                Y_trans.append(transformed[\"mask\"])\n",
    "                M_trans.append(transformed[\"mask0\"])\n",
    "        else:\n",
    "            X_trans, Y_trans, M_trans = X, Y, M\n",
    "\n",
    "        X_trans, Y_trans, M_trans = (\n",
    "            np.stack(X_trans, axis=0),\n",
    "            np.stack(Y_trans, axis=0),\n",
    "            np.stack(M_trans, axis=0),\n",
    "        )\n",
    "        if Y_trans.ndim != 4:\n",
    "            Y_trans = np.expand_dims(Y_trans, -1)\n",
    "        if M_trans.ndim != 4:\n",
    "            M_trans = np.expand_dims(M_trans, -1)\n",
    "        return (X_trans, M_trans), Y_trans\n",
    "\n",
    "\n",
    "epochs = 500\n",
    "\n",
    "train_gen = AugmentedSequence(\n",
    "    x_train_files,\n",
    "    y_train_files,\n",
    "    fov_train_files,\n",
    "    batch_size,\n",
    "    patch_size,\n",
    "    seed,\n",
    "    cycles=1,\n",
    ")\n",
    "\"\"\"\n",
    "train_gen = AugmentedSequence(\n",
    "    train_dataset.x_paths,\n",
    "    train_dataset.y_paths,\n",
    "    train_dataset.m_paths,\n",
    "    batch_size,\n",
    "    patch_size,\n",
    "    seed,\n",
    "    augment=False,\n",
    "    cycles=1,\n",
    ")\n",
    "\"\"\"\n",
    "# train_gen = PatchedSequence(x_train_files, y_train_files, batch_size, patch_size, seed)\n",
    "\n",
    "val_gen = AugmentedSequence(\n",
    "    val_dataset.x_paths,\n",
    "    val_dataset.y_paths,\n",
    "    val_dataset.m_paths,\n",
    "    batch_size,\n",
    "    patch_size,\n",
    "    seed,\n",
    "    False,\n",
    ")\n",
    "test_gen = AugmentedSequence(\n",
    "    test_dataset.x_paths,\n",
    "    test_dataset.y_paths,\n",
    "    test_dataset.m_paths,\n",
    "    1,\n",
    "    None,\n",
    "    seed,\n",
    "    False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 640, 640, 3)\n",
      "(1, 640, 640, 1)\n",
      "(1, 640, 640, 1)\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 6;\n",
       "                var nbb_unformatted_code = \"print(train_gen[0][0][0].shape)\\nprint(train_gen[0][0][1].shape)\\nprint(train_gen[0][1].shape)\";\n",
       "                var nbb_formatted_code = \"print(train_gen[0][0][0].shape)\\nprint(train_gen[0][0][1].shape)\\nprint(train_gen[0][1].shape)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(train_gen[0][0][0].shape)\n",
    "print(train_gen[0][0][1].shape)\n",
    "print(train_gen[0][1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 7;\n",
       "                var nbb_unformatted_code = \"# model = sm.Unet(backbone_name='resnet34', input_shape=(patch_size, patch_size, 3), classes=1, encoder_freeze=True)\\n# model.summary()\\n\\n\\ndef modified_sm_unet(model, patch_size):\\n    masks = Input(shape=(patch_size, patch_size, 1))\\n    inputs = model.input\\n    outputs = model.output\\n    new_output = Multiply()([outputs, masks])\\n    new_model = Model(inputs=[inputs, masks], outputs=new_output)\\n\\n    return new_model\";\n",
       "                var nbb_formatted_code = \"# model = sm.Unet(backbone_name='resnet34', input_shape=(patch_size, patch_size, 3), classes=1, encoder_freeze=True)\\n# model.summary()\\n\\n\\ndef modified_sm_unet(model, patch_size):\\n    masks = Input(shape=(patch_size, patch_size, 1))\\n    inputs = model.input\\n    outputs = model.output\\n    new_output = Multiply()([outputs, masks])\\n    new_model = Model(inputs=[inputs, masks], outputs=new_output)\\n\\n    return new_model\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = sm.Unet(backbone_name='resnet34', input_shape=(patch_size, patch_size, 3), classes=1, encoder_freeze=True)\n",
    "# model.summary()\n",
    "\n",
    "\n",
    "def modified_sm_unet(model, patch_size):\n",
    "    masks = Input(shape=(patch_size, patch_size, 1))\n",
    "    inputs = model.input\n",
    "    outputs = model.output\n",
    "    new_output = Multiply()([outputs, masks])\n",
    "    new_model = Model(inputs=[inputs, masks], outputs=new_output)\n",
    "\n",
    "    return new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 8;\n",
       "                var nbb_unformatted_code = \"# %tensorboard --logdir f\\\"./logs-{chp_tmp}/scalars\\\"\";\n",
       "                var nbb_formatted_code = \"# %tensorboard --logdir f\\\"./logs-{chp_tmp}/scalars\\\"\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %tensorboard --logdir f\"./logs-{chp_tmp}/scalars\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_checkpoint-2021-May-24_16:40:20.h5\n",
      "best_checkpoint='resnet50_augmented_adam_bce_jaccard_best.h5'\n",
      "best_checkpoint_g_mean='resnet50_augmented_adam_bce_jaccard_best_g_mean.h5'\n",
      "last_checkpoint='resnet50_augmented_adam_bce_jaccard_last.h5'\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 9;\n",
       "                var nbb_unformatted_code = \"import datetime\\n\\n\\ndef get_ftime():\\n    return datetime.datetime.today().strftime(\\\"%Y-%B-%d_%H:%M:%S\\\")\\n\\n\\ndef get_model_checkpoint():\\n    return f\\\"model_checkpoint-{get_ftime()}.h5\\\"\\n\\n\\nmodel_checkpoint = get_model_checkpoint()\\n\\nprint(model_checkpoint)\\n\\nfrom tensorflow.keras import backend as K\\n\\n\\nclass LRTensorBoard(TensorBoard):\\n    def __init__(\\n        self, log_dir, **kwargs\\n    ):  # add other arguments to __init__ if you need\\n        super().__init__(log_dir=log_dir, **kwargs)\\n\\n    def on_epoch_end(self, epoch, logs=None):\\n        logs = logs or {}\\n        logs.update({\\\"lr\\\": K.eval(self.model.optimizer.lr)})\\n        super().on_epoch_end(epoch, logs)\\n\\n\\ndef scheduler(epoch, lr):\\n    if epoch < 200:\\n        return 1e-4\\n    else:\\n        return lr * tf.math.exp(-0.1)\\n\\n\\nchp_tmp = \\\"resnet50_augmented_adam_bce_jaccard_{}.h5\\\"\\nbest_checkpoint = chp_tmp.format(\\\"best\\\")\\nbest_checkpoint_g_mean = chp_tmp.format(\\\"best_g_mean\\\")\\nlast_checkpoint = chp_tmp.format(\\\"last\\\")\\nprint(f\\\"{best_checkpoint=}\\\")\\nprint(f\\\"{best_checkpoint_g_mean=}\\\")\\nprint(f\\\"{last_checkpoint=}\\\")\";\n",
       "                var nbb_formatted_code = \"import datetime\\n\\n\\ndef get_ftime():\\n    return datetime.datetime.today().strftime(\\\"%Y-%B-%d_%H:%M:%S\\\")\\n\\n\\ndef get_model_checkpoint():\\n    return f\\\"model_checkpoint-{get_ftime()}.h5\\\"\\n\\n\\nmodel_checkpoint = get_model_checkpoint()\\n\\nprint(model_checkpoint)\\n\\nfrom tensorflow.keras import backend as K\\n\\n\\nclass LRTensorBoard(TensorBoard):\\n    def __init__(\\n        self, log_dir, **kwargs\\n    ):  # add other arguments to __init__ if you need\\n        super().__init__(log_dir=log_dir, **kwargs)\\n\\n    def on_epoch_end(self, epoch, logs=None):\\n        logs = logs or {}\\n        logs.update({\\\"lr\\\": K.eval(self.model.optimizer.lr)})\\n        super().on_epoch_end(epoch, logs)\\n\\n\\ndef scheduler(epoch, lr):\\n    if epoch < 200:\\n        return 1e-4\\n    else:\\n        return lr * tf.math.exp(-0.1)\\n\\n\\nchp_tmp = \\\"resnet50_augmented_adam_bce_jaccard_{}.h5\\\"\\nbest_checkpoint = chp_tmp.format(\\\"best\\\")\\nbest_checkpoint_g_mean = chp_tmp.format(\\\"best_g_mean\\\")\\nlast_checkpoint = chp_tmp.format(\\\"last\\\")\\nprint(f\\\"{best_checkpoint=}\\\")\\nprint(f\\\"{best_checkpoint_g_mean=}\\\")\\nprint(f\\\"{last_checkpoint=}\\\")\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "\n",
    "def get_ftime():\n",
    "    return datetime.datetime.today().strftime(\"%Y-%B-%d_%H:%M:%S\")\n",
    "\n",
    "\n",
    "def get_model_checkpoint():\n",
    "    return f\"model_checkpoint-{get_ftime()}.h5\"\n",
    "\n",
    "\n",
    "model_checkpoint = get_model_checkpoint()\n",
    "\n",
    "print(model_checkpoint)\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "\n",
    "class LRTensorBoard(TensorBoard):\n",
    "    def __init__(\n",
    "        self, log_dir, **kwargs\n",
    "    ):  # add other arguments to __init__ if you need\n",
    "        super().__init__(log_dir=log_dir, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        logs = logs or {}\n",
    "        logs.update({\"lr\": K.eval(self.model.optimizer.lr)})\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 200:\n",
    "        return 1e-4\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "\n",
    "chp_tmp = \"resnet50_augmented_adam_bce_jaccard_{}.h5\"\n",
    "best_checkpoint = chp_tmp.format(\"best\")\n",
    "best_checkpoint_g_mean = chp_tmp.format(\"best_g_mean\")\n",
    "last_checkpoint = chp_tmp.format(\"last\")\n",
    "print(f\"{best_checkpoint=}\")\n",
    "print(f\"{best_checkpoint_g_mean=}\")\n",
    "print(f\"{last_checkpoint=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 10;\n",
       "                var nbb_unformatted_code = \"keras.backend.clear_session()\\n\\noptimizer = Adam(lr=1e-4)\\n# optimizer = SGD(lr=1e-4)\\n\\n# loss = sm.losses.bce_jaccard_loss\\nloss = tversky_loss\\n# loss = sm.losses.dice_loss\\n\\nmetrics = [\\n    sm.metrics.IOUScore(threshold=0.5),\\n    sm.metrics.FScore(threshold=0.5),\\n    tf.keras.metrics.BinaryAccuracy(),\\n    sensitivity_metric,\\n    specificity_metric,\\n    g_mean_metric,\\n]\\n\\ncallbacks = [\\n    ModelCheckpoint(\\n        best_checkpoint, monitor=\\\"val_loss\\\", save_best_only=True, verbose=1\\n    ),\\n    ModelCheckpoint(\\n        best_checkpoint_g_mean,\\n        monitor=\\\"val_g_mean_metric\\\",\\n        save_best_only=True,\\n        verbose=1,\\n    ),\\n    ModelCheckpoint(last_checkpoint, save_best_only=False, verbose=0),\\n    ReduceLROnPlateau(\\n        monitor=\\\"val_loss\\\", factor=0.5, patience=50, min_lr=1e-6, verbose=1\\n    ),\\n    LRTensorBoard(log_dir=f\\\"./logs\\\"),\\n]\";\n",
       "                var nbb_formatted_code = \"keras.backend.clear_session()\\n\\noptimizer = Adam(lr=1e-4)\\n# optimizer = SGD(lr=1e-4)\\n\\n# loss = sm.losses.bce_jaccard_loss\\nloss = tversky_loss\\n# loss = sm.losses.dice_loss\\n\\nmetrics = [\\n    sm.metrics.IOUScore(threshold=0.5),\\n    sm.metrics.FScore(threshold=0.5),\\n    tf.keras.metrics.BinaryAccuracy(),\\n    sensitivity_metric,\\n    specificity_metric,\\n    g_mean_metric,\\n]\\n\\ncallbacks = [\\n    ModelCheckpoint(\\n        best_checkpoint, monitor=\\\"val_loss\\\", save_best_only=True, verbose=1\\n    ),\\n    ModelCheckpoint(\\n        best_checkpoint_g_mean,\\n        monitor=\\\"val_g_mean_metric\\\",\\n        save_best_only=True,\\n        verbose=1,\\n    ),\\n    ModelCheckpoint(last_checkpoint, save_best_only=False, verbose=0),\\n    ReduceLROnPlateau(\\n        monitor=\\\"val_loss\\\", factor=0.5, patience=50, min_lr=1e-6, verbose=1\\n    ),\\n    LRTensorBoard(log_dir=f\\\"./logs\\\"),\\n]\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "\n",
    "optimizer = Adam(lr=1e-4)\n",
    "# optimizer = SGD(lr=1e-4)\n",
    "\n",
    "# loss = sm.losses.bce_jaccard_loss\n",
    "loss = tversky_loss\n",
    "# loss = sm.losses.dice_loss\n",
    "\n",
    "metrics = [\n",
    "    sm.metrics.IOUScore(threshold=0.5),\n",
    "    sm.metrics.FScore(threshold=0.5),\n",
    "    tf.keras.metrics.BinaryAccuracy(),\n",
    "    sensitivity_metric,\n",
    "    specificity_metric,\n",
    "    g_mean_metric,\n",
    "]\n",
    "\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        best_checkpoint, monitor=\"val_loss\", save_best_only=True, verbose=1\n",
    "    ),\n",
    "    ModelCheckpoint(\n",
    "        best_checkpoint_g_mean,\n",
    "        monitor=\"val_g_mean_metric\",\n",
    "        save_best_only=True,\n",
    "        verbose=1,\n",
    "    ),\n",
    "    ModelCheckpoint(last_checkpoint, save_best_only=False, verbose=0),\n",
    "    ReduceLROnPlateau(\n",
    "        monitor=\"val_loss\", factor=0.5, patience=50, min_lr=1e-6, verbose=1\n",
    "    ),\n",
    "    LRTensorBoard(log_dir=f\"./logs\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "data (InputLayer)               [(None, 640, 640, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bn_data (BatchNormalization)    (None, 640, 640, 3)  9           data[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2D)  (None, 646, 646, 3)  0           bn_data[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv0 (Conv2D)                  (None, 320, 320, 64) 9408        zero_padding2d[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "bn0 (BatchNormalization)        (None, 320, 320, 64) 256         conv0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "relu0 (Activation)              (None, 320, 320, 64) 0           bn0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPadding2D (None, 322, 322, 64) 0           relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "pooling0 (MaxPooling2D)         (None, 160, 160, 64) 0           zero_padding2d_1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn1 (BatchNormaliz (None, 160, 160, 64) 256         pooling0[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu1 (Activation) (None, 160, 160, 64) 0           stage1_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv1 (Conv2D)     (None, 160, 160, 64) 4096        stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn2 (BatchNormaliz (None, 160, 160, 64) 256         stage1_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu2 (Activation) (None, 160, 160, 64) 0           stage1_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_2 (ZeroPadding2D (None, 162, 162, 64) 0           stage1_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv2 (Conv2D)     (None, 160, 160, 64) 36864       zero_padding2d_2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_bn3 (BatchNormaliz (None, 160, 160, 64) 256         stage1_unit1_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_relu3 (Activation) (None, 160, 160, 64) 0           stage1_unit1_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_conv3 (Conv2D)     (None, 160, 160, 256 16384       stage1_unit1_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit1_sc (Conv2D)        (None, 160, 160, 256 16384       stage1_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 160, 160, 256 0           stage1_unit1_conv3[0][0]         \n",
      "                                                                 stage1_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn1 (BatchNormaliz (None, 160, 160, 256 1024        add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu1 (Activation) (None, 160, 160, 256 0           stage1_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv1 (Conv2D)     (None, 160, 160, 64) 16384       stage1_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn2 (BatchNormaliz (None, 160, 160, 64) 256         stage1_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu2 (Activation) (None, 160, 160, 64) 0           stage1_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_3 (ZeroPadding2D (None, 162, 162, 64) 0           stage1_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv2 (Conv2D)     (None, 160, 160, 64) 36864       zero_padding2d_3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_bn3 (BatchNormaliz (None, 160, 160, 64) 256         stage1_unit2_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_relu3 (Activation) (None, 160, 160, 64) 0           stage1_unit2_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit2_conv3 (Conv2D)     (None, 160, 160, 256 16384       stage1_unit2_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 160, 160, 256 0           stage1_unit2_conv3[0][0]         \n",
      "                                                                 add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_bn1 (BatchNormaliz (None, 160, 160, 256 1024        add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_relu1 (Activation) (None, 160, 160, 256 0           stage1_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_conv1 (Conv2D)     (None, 160, 160, 64) 16384       stage1_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_bn2 (BatchNormaliz (None, 160, 160, 64) 256         stage1_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_relu2 (Activation) (None, 160, 160, 64) 0           stage1_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_4 (ZeroPadding2D (None, 162, 162, 64) 0           stage1_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_conv2 (Conv2D)     (None, 160, 160, 64) 36864       zero_padding2d_4[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_bn3 (BatchNormaliz (None, 160, 160, 64) 256         stage1_unit3_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_relu3 (Activation) (None, 160, 160, 64) 0           stage1_unit3_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage1_unit3_conv3 (Conv2D)     (None, 160, 160, 256 16384       stage1_unit3_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 160, 160, 256 0           stage1_unit3_conv3[0][0]         \n",
      "                                                                 add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn1 (BatchNormaliz (None, 160, 160, 256 1024        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu1 (Activation) (None, 160, 160, 256 0           stage2_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv1 (Conv2D)     (None, 160, 160, 128 32768       stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn2 (BatchNormaliz (None, 160, 160, 128 512         stage2_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu2 (Activation) (None, 160, 160, 128 0           stage2_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_5 (ZeroPadding2D (None, 162, 162, 128 0           stage2_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv2 (Conv2D)     (None, 80, 80, 128)  147456      zero_padding2d_5[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_bn3 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit1_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_relu3 (Activation) (None, 80, 80, 128)  0           stage2_unit1_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_conv3 (Conv2D)     (None, 80, 80, 512)  65536       stage2_unit1_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit1_sc (Conv2D)        (None, 80, 80, 512)  131072      stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 80, 80, 512)  0           stage2_unit1_conv3[0][0]         \n",
      "                                                                 stage2_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn1 (BatchNormaliz (None, 80, 80, 512)  2048        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu1 (Activation) (None, 80, 80, 512)  0           stage2_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv1 (Conv2D)     (None, 80, 80, 128)  65536       stage2_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn2 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu2 (Activation) (None, 80, 80, 128)  0           stage2_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_6 (ZeroPadding2D (None, 82, 82, 128)  0           stage2_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv2 (Conv2D)     (None, 80, 80, 128)  147456      zero_padding2d_6[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_bn3 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit2_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_relu3 (Activation) (None, 80, 80, 128)  0           stage2_unit2_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit2_conv3 (Conv2D)     (None, 80, 80, 512)  65536       stage2_unit2_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 80, 80, 512)  0           stage2_unit2_conv3[0][0]         \n",
      "                                                                 add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_bn1 (BatchNormaliz (None, 80, 80, 512)  2048        add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_relu1 (Activation) (None, 80, 80, 512)  0           stage2_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_conv1 (Conv2D)     (None, 80, 80, 128)  65536       stage2_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_bn2 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_relu2 (Activation) (None, 80, 80, 128)  0           stage2_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_7 (ZeroPadding2D (None, 82, 82, 128)  0           stage2_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_conv2 (Conv2D)     (None, 80, 80, 128)  147456      zero_padding2d_7[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_bn3 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit3_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_relu3 (Activation) (None, 80, 80, 128)  0           stage2_unit3_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit3_conv3 (Conv2D)     (None, 80, 80, 512)  65536       stage2_unit3_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 80, 80, 512)  0           stage2_unit3_conv3[0][0]         \n",
      "                                                                 add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_bn1 (BatchNormaliz (None, 80, 80, 512)  2048        add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_relu1 (Activation) (None, 80, 80, 512)  0           stage2_unit4_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_conv1 (Conv2D)     (None, 80, 80, 128)  65536       stage2_unit4_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_bn2 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit4_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_relu2 (Activation) (None, 80, 80, 128)  0           stage2_unit4_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_8 (ZeroPadding2D (None, 82, 82, 128)  0           stage2_unit4_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_conv2 (Conv2D)     (None, 80, 80, 128)  147456      zero_padding2d_8[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_bn3 (BatchNormaliz (None, 80, 80, 128)  512         stage2_unit4_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_relu3 (Activation) (None, 80, 80, 128)  0           stage2_unit4_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage2_unit4_conv3 (Conv2D)     (None, 80, 80, 512)  65536       stage2_unit4_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 80, 80, 512)  0           stage2_unit4_conv3[0][0]         \n",
      "                                                                 add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn1 (BatchNormaliz (None, 80, 80, 512)  2048        add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu1 (Activation) (None, 80, 80, 512)  0           stage3_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv1 (Conv2D)     (None, 80, 80, 256)  131072      stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn2 (BatchNormaliz (None, 80, 80, 256)  1024        stage3_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu2 (Activation) (None, 80, 80, 256)  0           stage3_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_9 (ZeroPadding2D (None, 82, 82, 256)  0           stage3_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv2 (Conv2D)     (None, 40, 40, 256)  589824      zero_padding2d_9[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_bn3 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit1_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_relu3 (Activation) (None, 40, 40, 256)  0           stage3_unit1_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_conv3 (Conv2D)     (None, 40, 40, 1024) 262144      stage3_unit1_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit1_sc (Conv2D)        (None, 40, 40, 1024) 524288      stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 40, 40, 1024) 0           stage3_unit1_conv3[0][0]         \n",
      "                                                                 stage3_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn1 (BatchNormaliz (None, 40, 40, 1024) 4096        add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu1 (Activation) (None, 40, 40, 1024) 0           stage3_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv1 (Conv2D)     (None, 40, 40, 256)  262144      stage3_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn2 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu2 (Activation) (None, 40, 40, 256)  0           stage3_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_10 (ZeroPadding2 (None, 42, 42, 256)  0           stage3_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv2 (Conv2D)     (None, 40, 40, 256)  589824      zero_padding2d_10[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_bn3 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit2_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_relu3 (Activation) (None, 40, 40, 256)  0           stage3_unit2_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit2_conv3 (Conv2D)     (None, 40, 40, 1024) 262144      stage3_unit2_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 40, 40, 1024) 0           stage3_unit2_conv3[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_bn1 (BatchNormaliz (None, 40, 40, 1024) 4096        add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_relu1 (Activation) (None, 40, 40, 1024) 0           stage3_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_conv1 (Conv2D)     (None, 40, 40, 256)  262144      stage3_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_bn2 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_relu2 (Activation) (None, 40, 40, 256)  0           stage3_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_11 (ZeroPadding2 (None, 42, 42, 256)  0           stage3_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_conv2 (Conv2D)     (None, 40, 40, 256)  589824      zero_padding2d_11[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_bn3 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit3_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_relu3 (Activation) (None, 40, 40, 256)  0           stage3_unit3_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit3_conv3 (Conv2D)     (None, 40, 40, 1024) 262144      stage3_unit3_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 40, 40, 1024) 0           stage3_unit3_conv3[0][0]         \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_bn1 (BatchNormaliz (None, 40, 40, 1024) 4096        add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_relu1 (Activation) (None, 40, 40, 1024) 0           stage3_unit4_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_conv1 (Conv2D)     (None, 40, 40, 256)  262144      stage3_unit4_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_bn2 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit4_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_relu2 (Activation) (None, 40, 40, 256)  0           stage3_unit4_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_12 (ZeroPadding2 (None, 42, 42, 256)  0           stage3_unit4_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_conv2 (Conv2D)     (None, 40, 40, 256)  589824      zero_padding2d_12[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_bn3 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit4_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_relu3 (Activation) (None, 40, 40, 256)  0           stage3_unit4_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit4_conv3 (Conv2D)     (None, 40, 40, 1024) 262144      stage3_unit4_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 40, 40, 1024) 0           stage3_unit4_conv3[0][0]         \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_bn1 (BatchNormaliz (None, 40, 40, 1024) 4096        add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_relu1 (Activation) (None, 40, 40, 1024) 0           stage3_unit5_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_conv1 (Conv2D)     (None, 40, 40, 256)  262144      stage3_unit5_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_bn2 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit5_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_relu2 (Activation) (None, 40, 40, 256)  0           stage3_unit5_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_13 (ZeroPadding2 (None, 42, 42, 256)  0           stage3_unit5_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_conv2 (Conv2D)     (None, 40, 40, 256)  589824      zero_padding2d_13[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_bn3 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit5_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_relu3 (Activation) (None, 40, 40, 256)  0           stage3_unit5_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit5_conv3 (Conv2D)     (None, 40, 40, 1024) 262144      stage3_unit5_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 40, 40, 1024) 0           stage3_unit5_conv3[0][0]         \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_bn1 (BatchNormaliz (None, 40, 40, 1024) 4096        add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_relu1 (Activation) (None, 40, 40, 1024) 0           stage3_unit6_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_conv1 (Conv2D)     (None, 40, 40, 256)  262144      stage3_unit6_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_bn2 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit6_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_relu2 (Activation) (None, 40, 40, 256)  0           stage3_unit6_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_14 (ZeroPadding2 (None, 42, 42, 256)  0           stage3_unit6_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_conv2 (Conv2D)     (None, 40, 40, 256)  589824      zero_padding2d_14[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_bn3 (BatchNormaliz (None, 40, 40, 256)  1024        stage3_unit6_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_relu3 (Activation) (None, 40, 40, 256)  0           stage3_unit6_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage3_unit6_conv3 (Conv2D)     (None, 40, 40, 1024) 262144      stage3_unit6_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 40, 40, 1024) 0           stage3_unit6_conv3[0][0]         \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn1 (BatchNormaliz (None, 40, 40, 1024) 4096        add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu1 (Activation) (None, 40, 40, 1024) 0           stage4_unit1_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv1 (Conv2D)     (None, 40, 40, 512)  524288      stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn2 (BatchNormaliz (None, 40, 40, 512)  2048        stage4_unit1_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu2 (Activation) (None, 40, 40, 512)  0           stage4_unit1_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_15 (ZeroPadding2 (None, 42, 42, 512)  0           stage4_unit1_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv2 (Conv2D)     (None, 20, 20, 512)  2359296     zero_padding2d_15[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_bn3 (BatchNormaliz (None, 20, 20, 512)  2048        stage4_unit1_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_relu3 (Activation) (None, 20, 20, 512)  0           stage4_unit1_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_conv3 (Conv2D)     (None, 20, 20, 2048) 1048576     stage4_unit1_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit1_sc (Conv2D)        (None, 20, 20, 2048) 2097152     stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 20, 20, 2048) 0           stage4_unit1_conv3[0][0]         \n",
      "                                                                 stage4_unit1_sc[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn1 (BatchNormaliz (None, 20, 20, 2048) 8192        add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu1 (Activation) (None, 20, 20, 2048) 0           stage4_unit2_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv1 (Conv2D)     (None, 20, 20, 512)  1048576     stage4_unit2_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn2 (BatchNormaliz (None, 20, 20, 512)  2048        stage4_unit2_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu2 (Activation) (None, 20, 20, 512)  0           stage4_unit2_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_16 (ZeroPadding2 (None, 22, 22, 512)  0           stage4_unit2_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv2 (Conv2D)     (None, 20, 20, 512)  2359296     zero_padding2d_16[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_bn3 (BatchNormaliz (None, 20, 20, 512)  2048        stage4_unit2_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_relu3 (Activation) (None, 20, 20, 512)  0           stage4_unit2_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit2_conv3 (Conv2D)     (None, 20, 20, 2048) 1048576     stage4_unit2_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 20, 20, 2048) 0           stage4_unit2_conv3[0][0]         \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_bn1 (BatchNormaliz (None, 20, 20, 2048) 8192        add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_relu1 (Activation) (None, 20, 20, 2048) 0           stage4_unit3_bn1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_conv1 (Conv2D)     (None, 20, 20, 512)  1048576     stage4_unit3_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_bn2 (BatchNormaliz (None, 20, 20, 512)  2048        stage4_unit3_conv1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_relu2 (Activation) (None, 20, 20, 512)  0           stage4_unit3_bn2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_17 (ZeroPadding2 (None, 22, 22, 512)  0           stage4_unit3_relu2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_conv2 (Conv2D)     (None, 20, 20, 512)  2359296     zero_padding2d_17[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_bn3 (BatchNormaliz (None, 20, 20, 512)  2048        stage4_unit3_conv2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_relu3 (Activation) (None, 20, 20, 512)  0           stage4_unit3_bn3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "stage4_unit3_conv3 (Conv2D)     (None, 20, 20, 2048) 1048576     stage4_unit3_relu3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 20, 20, 2048) 0           stage4_unit3_conv3[0][0]         \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bn1 (BatchNormalization)        (None, 20, 20, 2048) 8192        add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "relu1 (Activation)              (None, 20, 20, 2048) 0           bn1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_upsampling (UpSa (None, 40, 40, 2048) 0           relu1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0_concat (Concaten (None, 40, 40, 3072) 0           decoder_stage0_upsampling[0][0]  \n",
      "                                                                 stage4_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_conv (Conv2D)   (None, 40, 40, 256)  7077888     decoder_stage0_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_bn (BatchNormal (None, 40, 40, 256)  1024        decoder_stage0a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0a_relu (Activatio (None, 40, 40, 256)  0           decoder_stage0a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_conv (Conv2D)   (None, 40, 40, 256)  589824      decoder_stage0a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_bn (BatchNormal (None, 40, 40, 256)  1024        decoder_stage0b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage0b_relu (Activatio (None, 40, 40, 256)  0           decoder_stage0b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_upsampling (UpSa (None, 80, 80, 256)  0           decoder_stage0b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1_concat (Concaten (None, 80, 80, 768)  0           decoder_stage1_upsampling[0][0]  \n",
      "                                                                 stage3_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_conv (Conv2D)   (None, 80, 80, 128)  884736      decoder_stage1_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_bn (BatchNormal (None, 80, 80, 128)  512         decoder_stage1a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1a_relu (Activatio (None, 80, 80, 128)  0           decoder_stage1a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_conv (Conv2D)   (None, 80, 80, 128)  147456      decoder_stage1a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_bn (BatchNormal (None, 80, 80, 128)  512         decoder_stage1b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage1b_relu (Activatio (None, 80, 80, 128)  0           decoder_stage1b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_upsampling (UpSa (None, 160, 160, 128 0           decoder_stage1b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2_concat (Concaten (None, 160, 160, 384 0           decoder_stage2_upsampling[0][0]  \n",
      "                                                                 stage2_unit1_relu1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_conv (Conv2D)   (None, 160, 160, 64) 221184      decoder_stage2_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_bn (BatchNormal (None, 160, 160, 64) 256         decoder_stage2a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2a_relu (Activatio (None, 160, 160, 64) 0           decoder_stage2a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_conv (Conv2D)   (None, 160, 160, 64) 36864       decoder_stage2a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_bn (BatchNormal (None, 160, 160, 64) 256         decoder_stage2b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage2b_relu (Activatio (None, 160, 160, 64) 0           decoder_stage2b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_upsampling (UpSa (None, 320, 320, 64) 0           decoder_stage2b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3_concat (Concaten (None, 320, 320, 128 0           decoder_stage3_upsampling[0][0]  \n",
      "                                                                 relu0[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_conv (Conv2D)   (None, 320, 320, 32) 36864       decoder_stage3_concat[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_bn (BatchNormal (None, 320, 320, 32) 128         decoder_stage3a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3a_relu (Activatio (None, 320, 320, 32) 0           decoder_stage3a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_conv (Conv2D)   (None, 320, 320, 32) 9216        decoder_stage3a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_bn (BatchNormal (None, 320, 320, 32) 128         decoder_stage3b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage3b_relu (Activatio (None, 320, 320, 32) 0           decoder_stage3b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4_upsampling (UpSa (None, 640, 640, 32) 0           decoder_stage3b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_conv (Conv2D)   (None, 640, 640, 16) 4608        decoder_stage4_upsampling[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_bn (BatchNormal (None, 640, 640, 16) 64          decoder_stage4a_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4a_relu (Activatio (None, 640, 640, 16) 0           decoder_stage4a_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_conv (Conv2D)   (None, 640, 640, 16) 2304        decoder_stage4a_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_bn (BatchNormal (None, 640, 640, 16) 64          decoder_stage4b_conv[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "decoder_stage4b_relu (Activatio (None, 640, 640, 16) 0           decoder_stage4b_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "final_conv (Conv2D)             (None, 640, 640, 1)  145         decoder_stage4b_relu[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "sigmoid (Activation)            (None, 640, 640, 1)  0           final_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            [(None, 640, 640, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (None, 640, 640, 1)  0           sigmoid[0][0]                    \n",
      "                                                                 input_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 32,561,114\n",
      "Trainable params: 9,058,644\n",
      "Non-trainable params: 23,502,470\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33/33 [==============================] - 18s 263ms/step - loss: 0.7311 - iou_score: 0.1257 - f1-score: 0.2181 - binary_accuracy: 0.5285 - sensitivity_metric: 0.7114 - specificity_metric: 0.5083 - g_mean_metric: 0.5999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f6896a3aeb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 11;\n",
       "                var nbb_unformatted_code = \"# model = unet(input_shape=(patch_size, patch_size, 3), classes=1)\\n\\nmodel = sm.Unet(\\n    \\\"resnet50\\\", (patch_size, patch_size, 3), activation=\\\"sigmoid\\\", encoder_freeze=True\\n)\\n\\nmodel = modified_sm_unet(model, patch_size)\\nmodel.summary()\\n\\nmodel.compile(optimizer, loss=loss, metrics=metrics)\\nmodel.fit(train_gen, epochs=1, verbose=1)\";\n",
       "                var nbb_formatted_code = \"# model = unet(input_shape=(patch_size, patch_size, 3), classes=1)\\n\\nmodel = sm.Unet(\\n    \\\"resnet50\\\", (patch_size, patch_size, 3), activation=\\\"sigmoid\\\", encoder_freeze=True\\n)\\n\\nmodel = modified_sm_unet(model, patch_size)\\nmodel.summary()\\n\\nmodel.compile(optimizer, loss=loss, metrics=metrics)\\nmodel.fit(train_gen, epochs=1, verbose=1)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model = unet(input_shape=(patch_size, patch_size, 3), classes=1)\n",
    "\n",
    "model = sm.Unet(\n",
    "    \"resnet50\", (patch_size, patch_size, 3), activation=\"sigmoid\", encoder_freeze=True\n",
    ")\n",
    "\n",
    "model = modified_sm_unet(model, patch_size)\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "model.fit(train_gen, epochs=1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "33/33 [==============================] - 12s 367ms/step - loss: 0.6022 - iou_score: 0.2416 - f1-score: 0.3758 - binary_accuracy: 0.7390 - sensitivity_metric: 0.8589 - specificity_metric: 0.7284 - g_mean_metric: 0.7898 - val_loss: 0.7740 - val_iou_score: 0.0867 - val_f1-score: 0.1577 - val_binary_accuracy: 0.2273 - val_sensitivity_metric: 0.9994 - val_specificity_metric: 0.1575 - val_g_mean_metric: 0.2579\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.77398, saving model to resnet50_augmented_adam_bce_jaccard_best.h5\n",
      "\n",
      "Epoch 00001: val_g_mean_metric improved from inf to 0.25793, saving model to resnet50_augmented_adam_bce_jaccard_best_g_mean.h5\n",
      "Epoch 2/500\n",
      "33/33 [==============================] - 12s 356ms/step - loss: 0.5555 - iou_score: 0.3060 - f1-score: 0.4557 - binary_accuracy: 0.8288 - sensitivity_metric: 0.8630 - specificity_metric: 0.8265 - g_mean_metric: 0.8442 - val_loss: 0.7669 - val_iou_score: 0.0864 - val_f1-score: 0.1572 - val_binary_accuracy: 0.2406 - val_sensitivity_metric: 0.9781 - val_specificity_metric: 0.1737 - val_g_mean_metric: 0.3300\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.77398 to 0.76686, saving model to resnet50_augmented_adam_bce_jaccard_best.h5\n",
      "\n",
      "Epoch 00002: val_g_mean_metric did not improve from 0.25793\n",
      "Epoch 3/500\n",
      "33/33 [==============================] - 11s 338ms/step - loss: 0.5336 - iou_score: 0.3314 - f1-score: 0.4823 - binary_accuracy: 0.8646 - sensitivity_metric: 0.8416 - specificity_metric: 0.8673 - g_mean_metric: 0.8536 - val_loss: 0.7812 - val_iou_score: 0.0858 - val_f1-score: 0.1562 - val_binary_accuracy: 0.2648 - val_sensitivity_metric: 0.9415 - val_specificity_metric: 0.2035 - val_g_mean_metric: 0.3865\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 0.76686\n",
      "\n",
      "Epoch 00003: val_g_mean_metric did not improve from 0.25793\n",
      "Epoch 4/500\n",
      "33/33 [==============================] - 11s 325ms/step - loss: 0.4922 - iou_score: 0.3834 - f1-score: 0.5346 - binary_accuracy: 0.8891 - sensitivity_metric: 0.8390 - specificity_metric: 0.8932 - g_mean_metric: 0.8651 - val_loss: 0.8233 - val_iou_score: 0.0556 - val_f1-score: 0.1045 - val_binary_accuracy: 0.8183 - val_sensitivity_metric: 0.1556 - val_specificity_metric: 0.8725 - val_g_mean_metric: 0.3599\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 0.76686\n",
      "\n",
      "Epoch 00004: val_g_mean_metric did not improve from 0.25793\n",
      "Epoch 5/500\n",
      "33/33 [==============================] - 11s 325ms/step - loss: 0.4518 - iou_score: 0.4154 - f1-score: 0.5709 - binary_accuracy: 0.8974 - sensitivity_metric: 0.8100 - specificity_metric: 0.9057 - g_mean_metric: 0.8552 - val_loss: 0.7654 - val_iou_score: 0.0867 - val_f1-score: 0.1577 - val_binary_accuracy: 0.2305 - val_sensitivity_metric: 0.9948 - val_specificity_metric: 0.1613 - val_g_mean_metric: 0.2890\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.76686 to 0.76539, saving model to resnet50_augmented_adam_bce_jaccard_best.h5\n",
      "\n",
      "Epoch 00005: val_g_mean_metric did not improve from 0.25793\n",
      "Epoch 6/500\n",
      "33/33 [==============================] - 11s 333ms/step - loss: 0.4352 - iou_score: 0.4220 - f1-score: 0.5818 - binary_accuracy: 0.9040 - sensitivity_metric: 0.8204 - specificity_metric: 0.9123 - g_mean_metric: 0.8643 - val_loss: 0.7654 - val_iou_score: 0.0866 - val_f1-score: 0.1576 - val_binary_accuracy: 0.2337 - val_sensitivity_metric: 0.9899 - val_specificity_metric: 0.1653 - val_g_mean_metric: 0.3062\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.76539\n",
      "\n",
      "Epoch 00006: val_g_mean_metric did not improve from 0.25793\n",
      "Epoch 7/500\n",
      "33/33 [==============================] - 11s 328ms/step - loss: 0.4309 - iou_score: 0.4257 - f1-score: 0.5789 - binary_accuracy: 0.9050 - sensitivity_metric: 0.7875 - specificity_metric: 0.9185 - g_mean_metric: 0.8494 - val_loss: 0.7651 - val_iou_score: 0.0867 - val_f1-score: 0.1577 - val_binary_accuracy: 0.2312 - val_sensitivity_metric: 0.9936 - val_specificity_metric: 0.1622 - val_g_mean_metric: 0.2933\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.76539 to 0.76508, saving model to resnet50_augmented_adam_bce_jaccard_best.h5\n",
      "\n",
      "Epoch 00007: val_g_mean_metric did not improve from 0.25793\n",
      "Epoch 8/500\n",
      "33/33 [==============================] - 11s 322ms/step - loss: 0.4036 - iou_score: 0.4422 - f1-score: 0.5936 - binary_accuracy: 0.9047 - sensitivity_metric: 0.8037 - specificity_metric: 0.9165 - g_mean_metric: 0.8572 - val_loss: 0.8985 - val_iou_score: 0.0073 - val_f1-score: 0.0144 - val_binary_accuracy: 0.9185 - val_sensitivity_metric: 0.0082 - val_specificity_metric: 0.9933 - val_g_mean_metric: 0.0764\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00008: val_g_mean_metric improved from 0.25793 to 0.07635, saving model to resnet50_augmented_adam_bce_jaccard_best_g_mean.h5\n",
      "Epoch 9/500\n",
      "33/33 [==============================] - 11s 330ms/step - loss: 0.3916 - iou_score: 0.4589 - f1-score: 0.6113 - binary_accuracy: 0.9137 - sensitivity_metric: 0.8035 - specificity_metric: 0.9262 - g_mean_metric: 0.8618 - val_loss: 0.7846 - val_iou_score: 0.0863 - val_f1-score: 0.1570 - val_binary_accuracy: 0.3000 - val_sensitivity_metric: 0.9019 - val_specificity_metric: 0.2449 - val_g_mean_metric: 0.4373\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00009: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 10/500\n",
      "33/33 [==============================] - 11s 339ms/step - loss: 0.3445 - iou_score: 0.5082 - f1-score: 0.6594 - binary_accuracy: 0.9228 - sensitivity_metric: 0.8125 - specificity_metric: 0.9359 - g_mean_metric: 0.8714 - val_loss: 0.7658 - val_iou_score: 0.0866 - val_f1-score: 0.1575 - val_binary_accuracy: 0.2448 - val_sensitivity_metric: 0.9739 - val_specificity_metric: 0.1786 - val_g_mean_metric: 0.3428\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00010: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 11/500\n",
      "33/33 [==============================] - 11s 322ms/step - loss: 0.3593 - iou_score: 0.4923 - f1-score: 0.6415 - binary_accuracy: 0.9338 - sensitivity_metric: 0.8305 - specificity_metric: 0.9433 - g_mean_metric: 0.8844 - val_loss: 0.7700 - val_iou_score: 0.0866 - val_f1-score: 0.1576 - val_binary_accuracy: 0.2929 - val_sensitivity_metric: 0.9157 - val_specificity_metric: 0.2361 - val_g_mean_metric: 0.4295\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00011: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 12/500\n",
      "33/33 [==============================] - 11s 342ms/step - loss: 0.3299 - iou_score: 0.5285 - f1-score: 0.6756 - binary_accuracy: 0.9347 - sensitivity_metric: 0.8078 - specificity_metric: 0.9485 - g_mean_metric: 0.8747 - val_loss: 0.7899 - val_iou_score: 0.0872 - val_f1-score: 0.1583 - val_binary_accuracy: 0.3577 - val_sensitivity_metric: 0.8329 - val_specificity_metric: 0.3137 - val_g_mean_metric: 0.4929\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00012: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 13/500\n",
      "33/33 [==============================] - 12s 358ms/step - loss: 0.3450 - iou_score: 0.4906 - f1-score: 0.6437 - binary_accuracy: 0.9275 - sensitivity_metric: 0.8131 - specificity_metric: 0.9391 - g_mean_metric: 0.8734 - val_loss: 0.8976 - val_iou_score: 0.0389 - val_f1-score: 0.0740 - val_binary_accuracy: 0.8732 - val_sensitivity_metric: 0.0724 - val_specificity_metric: 0.9386 - val_g_mean_metric: 0.2455\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00013: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 14/500\n",
      "33/33 [==============================] - 11s 343ms/step - loss: 0.3499 - iou_score: 0.4866 - f1-score: 0.6428 - binary_accuracy: 0.9337 - sensitivity_metric: 0.7972 - specificity_metric: 0.9470 - g_mean_metric: 0.8679 - val_loss: 0.7684 - val_iou_score: 0.0866 - val_f1-score: 0.1576 - val_binary_accuracy: 0.2731 - val_sensitivity_metric: 0.9379 - val_specificity_metric: 0.2124 - val_g_mean_metric: 0.4007\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00014: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 15/500\n",
      "33/33 [==============================] - 11s 338ms/step - loss: 0.3452 - iou_score: 0.4948 - f1-score: 0.6483 - binary_accuracy: 0.9363 - sensitivity_metric: 0.7847 - specificity_metric: 0.9507 - g_mean_metric: 0.8622 - val_loss: 0.9005 - val_iou_score: 0.0350 - val_f1-score: 0.0668 - val_binary_accuracy: 0.8827 - val_sensitivity_metric: 0.0601 - val_specificity_metric: 0.9499 - val_g_mean_metric: 0.2250\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00015: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00015: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 16/500\n",
      "33/33 [==============================] - 11s 331ms/step - loss: 0.3411 - iou_score: 0.4964 - f1-score: 0.6461 - binary_accuracy: 0.9320 - sensitivity_metric: 0.7999 - specificity_metric: 0.9464 - g_mean_metric: 0.8693 - val_loss: 0.9403 - val_iou_score: 0.0209 - val_f1-score: 0.0405 - val_binary_accuracy: 0.9066 - val_sensitivity_metric: 0.0277 - val_specificity_metric: 0.9787 - val_g_mean_metric: 0.1528\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00016: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 17/500\n",
      "33/33 [==============================] - 11s 330ms/step - loss: 0.3213 - iou_score: 0.5184 - f1-score: 0.6673 - binary_accuracy: 0.9349 - sensitivity_metric: 0.7949 - specificity_metric: 0.9506 - g_mean_metric: 0.8680 - val_loss: 0.7700 - val_iou_score: 0.0867 - val_f1-score: 0.1578 - val_binary_accuracy: 0.2563 - val_sensitivity_metric: 0.9599 - val_specificity_metric: 0.1921 - val_g_mean_metric: 0.3664\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00017: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 18/500\n",
      "33/33 [==============================] - 11s 323ms/step - loss: 0.2989 - iou_score: 0.5357 - f1-score: 0.6871 - binary_accuracy: 0.9391 - sensitivity_metric: 0.8143 - specificity_metric: 0.9525 - g_mean_metric: 0.8796 - val_loss: 0.7658 - val_iou_score: 0.0866 - val_f1-score: 0.1575 - val_binary_accuracy: 0.2472 - val_sensitivity_metric: 0.9713 - val_specificity_metric: 0.1814 - val_g_mean_metric: 0.3488\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00018: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 19/500\n",
      "33/33 [==============================] - 11s 327ms/step - loss: 0.3009 - iou_score: 0.5311 - f1-score: 0.6802 - binary_accuracy: 0.9357 - sensitivity_metric: 0.8132 - specificity_metric: 0.9502 - g_mean_metric: 0.8784 - val_loss: 0.7720 - val_iou_score: 0.0873 - val_f1-score: 0.1588 - val_binary_accuracy: 0.3249 - val_sensitivity_metric: 0.8826 - val_specificity_metric: 0.2740 - val_g_mean_metric: 0.4669\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00019: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 20/500\n",
      "33/33 [==============================] - 11s 335ms/step - loss: 0.2816 - iou_score: 0.5515 - f1-score: 0.7009 - binary_accuracy: 0.9418 - sensitivity_metric: 0.8195 - specificity_metric: 0.9540 - g_mean_metric: 0.8837 - val_loss: 0.7658 - val_iou_score: 0.0866 - val_f1-score: 0.1575 - val_binary_accuracy: 0.2491 - val_sensitivity_metric: 0.9683 - val_specificity_metric: 0.1838 - val_g_mean_metric: 0.3538\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.76508\n",
      "\n",
      "Epoch 00020: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 21/500\n",
      "33/33 [==============================] - 11s 332ms/step - loss: 0.2878 - iou_score: 0.5463 - f1-score: 0.6948 - binary_accuracy: 0.9359 - sensitivity_metric: 0.8071 - specificity_metric: 0.9511 - g_mean_metric: 0.8752 - val_loss: 0.7638 - val_iou_score: 0.0882 - val_f1-score: 0.1602 - val_binary_accuracy: 0.2841 - val_sensitivity_metric: 0.9405 - val_specificity_metric: 0.2248 - val_g_mean_metric: 0.4187\n",
      "\n",
      "Epoch 00021: val_loss improved from 0.76508 to 0.76381, saving model to resnet50_augmented_adam_bce_jaccard_best.h5\n",
      "\n",
      "Epoch 00021: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 22/500\n",
      "33/33 [==============================] - 11s 320ms/step - loss: 0.3109 - iou_score: 0.5193 - f1-score: 0.6694 - binary_accuracy: 0.9380 - sensitivity_metric: 0.8007 - specificity_metric: 0.9528 - g_mean_metric: 0.8729 - val_loss: 0.8665 - val_iou_score: 0.0570 - val_f1-score: 0.1068 - val_binary_accuracy: 0.8215 - val_sensitivity_metric: 0.1506 - val_specificity_metric: 0.8758 - val_g_mean_metric: 0.3552\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.76381\n",
      "\n",
      "Epoch 00022: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 23/500\n",
      "33/33 [==============================] - 11s 330ms/step - loss: 0.2688 - iou_score: 0.5563 - f1-score: 0.7072 - binary_accuracy: 0.9400 - sensitivity_metric: 0.8308 - specificity_metric: 0.9532 - g_mean_metric: 0.8893 - val_loss: 0.7680 - val_iou_score: 0.0913 - val_f1-score: 0.1650 - val_binary_accuracy: 0.3708 - val_sensitivity_metric: 0.8552 - val_specificity_metric: 0.3277 - val_g_mean_metric: 0.5105\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.76381\n",
      "\n",
      "Epoch 00023: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 24/500\n",
      "33/33 [==============================] - 11s 334ms/step - loss: 0.3093 - iou_score: 0.5210 - f1-score: 0.6737 - binary_accuracy: 0.9475 - sensitivity_metric: 0.7924 - specificity_metric: 0.9614 - g_mean_metric: 0.8721 - val_loss: 0.8188 - val_iou_score: 0.0710 - val_f1-score: 0.1308 - val_binary_accuracy: 0.7712 - val_sensitivity_metric: 0.2435 - val_specificity_metric: 0.8134 - val_g_mean_metric: 0.4376\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.76381\n",
      "\n",
      "Epoch 00024: val_g_mean_metric did not improve from 0.07635\n",
      "Epoch 25/500\n",
      "33/33 [==============================] - 11s 339ms/step - loss: 0.2780 - iou_score: 0.5585 - f1-score: 0.7022 - binary_accuracy: 0.9435 - sensitivity_metric: 0.8173 - specificity_metric: 0.9577 - g_mean_metric: 0.8841 - val_loss: 0.7684 - val_iou_score: 0.0885 - val_f1-score: 0.1607 - val_binary_accuracy: 0.3996 - val_sensitivity_metric: 0.7980 - val_specificity_metric: 0.3634 - val_g_mean_metric: 0.5253\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.76381\n",
      "\n",
      "Epoch 00025: val_g_mean_metric did not improve from 0.07635\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-403dc20af215>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# model.compile(optimizer, loss=loss, metrics=metrics)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m history = model.fit(\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m )\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1145\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1146\u001b[0m         \u001b[0mtraining_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'_supports_tf_logs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnumpy_logs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Only convert once.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'epoch'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1344\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_should_save_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\u001b[0m in \u001b[0;36m_save_model\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m   1406\u001b[0m                 filepath, overwrite=True, options=self._options)\n\u001b[1;32m   1407\u001b[0m           \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1408\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_remove_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \"\"\"\n\u001b[1;32m   2000\u001b[0m     \u001b[0;31m# pylint: enable=line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2001\u001b[0;31m     save.save_model(self, filepath, overwrite, include_optimizer, save_format,\n\u001b[0m\u001b[1;32m   2002\u001b[0m                     signatures, options, save_traces)\n\u001b[1;32m   2003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36msave_model\u001b[0;34m(model, filepath, overwrite, include_optimizer, save_format, signatures, options, save_traces)\u001b[0m\n\u001b[1;32m    151\u001b[0m           \u001b[0;34m'to the Tensorflow SavedModel format (by setting save_format=\"tf\") '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m           'or using `save_weights`.')\n\u001b[0;32m--> 153\u001b[0;31m     hdf5_format.save_model_to_hdf5(\n\u001b[0m\u001b[1;32m    154\u001b[0m         model, filepath, overwrite, include_optimizer)\n\u001b[1;32m    155\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/tensorflow/python/keras/saving/hdf5_format.py\u001b[0m in \u001b[0;36msave_model_to_hdf5\u001b[0;34m(model, filepath, overwrite, include_optimizer)\u001b[0m\n\u001b[1;32m    106\u001b[0m       \u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMakeDirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m     \u001b[0mopened_new_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[1;32m    404\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mphil\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m                 \u001b[0mfapl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_fapl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlibver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nslots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_nbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrdcc_w0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0m\u001b[1;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                                swmr=swmr)\n",
      "\u001b[0;32m~/Documents/Semestr6/IwM/unet/env/lib/python3.8/site-packages/h5py/_hl/files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[0;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_EXCL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mACC_TRUNC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfcpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Open in append mode (read/write).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.create\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/h5g.pyx\u001b[0m in \u001b[0;36mh5py.h5g.GroupID.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.8/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "            setTimeout(function() {\n",
       "                var nbb_cell_id = 12;\n",
       "                var nbb_unformatted_code = \"try:\\n    model.load_weights(model_checkpoint)\\nexcept:\\n    pass\\nfor layer in model.layers:\\n    layer.trainable = True\\n\\n# model.compile(optimizer, loss=loss, metrics=metrics)\\nhistory = model.fit(\\n    train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks, verbose=1\\n)\";\n",
       "                var nbb_formatted_code = \"try:\\n    model.load_weights(model_checkpoint)\\nexcept:\\n    pass\\nfor layer in model.layers:\\n    layer.trainable = True\\n\\n# model.compile(optimizer, loss=loss, metrics=metrics)\\nhistory = model.fit(\\n    train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks, verbose=1\\n)\";\n",
       "                var nbb_cells = Jupyter.notebook.get_cells();\n",
       "                for (var i = 0; i < nbb_cells.length; ++i) {\n",
       "                    if (nbb_cells[i].input_prompt_number == nbb_cell_id) {\n",
       "                        if (nbb_cells[i].get_text() == nbb_unformatted_code) {\n",
       "                             nbb_cells[i].set_text(nbb_formatted_code);\n",
       "                        }\n",
       "                        break;\n",
       "                    }\n",
       "                }\n",
       "            }, 500);\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "try:\n",
    "    model.load_weights(model_checkpoint)\n",
    "except:\n",
    "    pass\n",
    "for layer in model.layers:\n",
    "    layer.trainable = True\n",
    "\n",
    "# model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "history = model.fit(\n",
    "    train_gen, epochs=epochs, validation_data=val_gen, callbacks=callbacks, verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "import matplotlib.pyplot as plt\n",
    "model.load_weights(best_checkpoint)\n",
    "(x_to_predict, m), y_to_predict = val_gen[4]\n",
    "pred = model.predict([x_to_predict, m])\n",
    "print(x_to_predict.shape, y_to_predict.shape, pred.shape)\n",
    "plot_imgs(prepare_for_pyplot(x_to_predict), y_to_predict, pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(best_checkpoint)\n",
    "(x_to_predict, m), y_to_predict = val_gen[5]\n",
    "pred = model.predict([x_to_predict, m])\n",
    "print(x_to_predict.shape, y_to_predict.shape, pred.shape)\n",
    "plot_imgs(prepare_for_pyplot(x_to_predict), y_to_predict, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "path_x = \"/home/maciej/.keras/datasets/images/05_g.jpg\"\n",
    "path_y = \"/home/maciej/.keras/datasets/manual1/05_g.tif\"\n",
    "path_m = \"/home/maciej/.keras/datasets/mask/05_g_mask.tif\"\n",
    "\n",
    "# model = sm.Unet(backbone_name='resnet34', input_shape=(patch_size, patch_size, 3), classes=1, encoder_freeze=False)\n",
    "# model = modified_sm_unet(model, patch_size)\n",
    "# model.load_weights('augmented_adam_focal_best.h5')\n",
    "# model.load_weights('resnet_34_augmented_adam_tversky_best-Copy1.h5')\n",
    "\n",
    "model.load_weights(\"resnet_34_augmented_adam1e4_tversky_best-Copy1.h5\")\n",
    "x_whole = np.array(load_img(path_x))\n",
    "y_whole = np.array(load_img(path_y, color_mode=\"grayscale\"))\n",
    "m = np.array(load_img(path_m, color_mode=\"grayscale\"))\n",
    "# m = np.ones_like(m)\n",
    "x_whole = x_whole.astype(\"float32\") / 255\n",
    "y_whole = np.clip(y_whole, 0, 1).astype(\"float32\")\n",
    "m = np.clip(m, 0, 1).astype(\"float32\")\n",
    "\n",
    "# print(mask.shape, m.shape)\n",
    "mask = ~binary_erosion(m, disk(2))\n",
    "m[mask] = 0\n",
    "\n",
    "x_whole, y_whole, m = (\n",
    "    np.expand_dims(x_whole, 0),\n",
    "    np.expand_dims(y_whole, [0, -1]),\n",
    "    np.expand_dims(m, [0, -1]),\n",
    ")\n",
    "\n",
    "\n",
    "pred = get_predictions_from_patches(\n",
    "    model, x_whole, m, patch_size, patch_size, threshold=None\n",
    ")\n",
    "print(x_whole.shape, y_whole.shape, m.shape, pred.shape)\n",
    "plot_imgs(prepare_for_pyplot(x_whole), y_whole, pred)\n",
    "\n",
    "print(np.squeeze(pred[0], -1).shape)\n",
    "img = prepare_for_pyplot(np.squeeze(pred[0], -1))\n",
    "im = Image.fromarray(img * 255)\n",
    "# im.save(\"pred.jpg\")\n",
    "im.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "threshold = 0.14\n",
    "\n",
    "save_dir = f\"predictions_na_5_thr={threshold}\"\n",
    "try:\n",
    "    os.mkdir(save_dir)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "\n",
    "for x_test_file, y_test_file, m_file in zip(x_test_files, y_test_files, fov_test_files):\n",
    "    file_name = x_test_file.split(\"/\")[-1].split(\".\")[0]\n",
    "\n",
    "    x_whole = np.array(load_img(x_test_file))\n",
    "    y_whole = np.array(load_img(y_test_file, color_mode=\"grayscale\"))\n",
    "    m = np.array(load_img(m_file, color_mode=\"grayscale\"))\n",
    "\n",
    "    x_whole = x_whole.astype(\"float32\") / 255\n",
    "    y_whole = np.clip(y_whole, 0, 1).astype(\"float32\")\n",
    "    m = np.clip(m, 0, 1).astype(\"float32\")\n",
    "\n",
    "    x_whole, y_whole, m = (\n",
    "        np.expand_dims(x_whole, 0),\n",
    "        np.expand_dims(y_whole, [0, -1]),\n",
    "        np.expand_dims(m, [0, -1]),\n",
    "    )\n",
    "\n",
    "    pred = get_predictions_from_patches(\n",
    "        model, x_whole, m, patch_size, patch_size, threshold=threshold\n",
    "    )\n",
    "    pred = np.squeeze(pred[0], -1)\n",
    "    im = Image.fromarray(np.uint8(pred * 255))\n",
    "    im.save(f\"{save_dir}/{file_name}-p.png\")\n",
    "\n",
    "    x_whole = x_whole[0]\n",
    "    x_whole[pred == 1] = np.array([0, 1, 0])\n",
    "    im = Image.fromarray(np.uint8(x_whole * 255))\n",
    "    im.save(f\"{save_dir}/{file_name}-o.png\")\n",
    "\n",
    "    y_whole = np.squeeze(y_whole, (0, 3))\n",
    "    im = Image.fromarray(np.uint8(y_whole * 255))\n",
    "    im.save(f\"{save_dir}/{file_name}-m.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from myutils.metrics import prediction_report\n",
    "import pprint\n",
    "import pandas as pd\n",
    "\n",
    "def find_best_threshold(model, train_seq, patch_size, stride, target_metric='accuracy', steps=10):\n",
    "    def helper(left_thr, right_thr, left_metric, right_metric, steps):\n",
    "        thr = (left_thr + right_thr) / 2\n",
    "        report = prediction_report(model, train_seq, patch_size=patch_size, stride=patch_size, threshold=thr)\n",
    "        metric = report[1][target_metric]\n",
    "        if steps == 0:\n",
    "            return max((left_metric, left_thr), (right_metric, right_thr), (metric, thr))\n",
    "        else:\n",
    "            if metric > left_metric:\n",
    "                return helper(thr, right_thr, metric, right_metric, steps-1)\n",
    "            else:\n",
    "                return helper(left_thr, left_metric, thr, metric, steps-1)\n",
    "    report = prediction_report(model, train_seq, patch_size=patch_size, stride=patch_size, threshold=0.05)\n",
    "    left_met = report[1][target_metric]\n",
    "    report = prediction_report(model, train_seq, patch_size=patch_size, stride=patch_size, threshold=1-0.05)\n",
    "    right_met = report[1][target_metric]\n",
    "    \n",
    "    return helper(0.05, 1-0.05, left_met, right_met, steps)\n",
    "\n",
    "def linear_search(model, train_seq, patch_size, stride, steps=10):\n",
    "    thresholds = np.linspace(0.05, 1-0.05, steps)\n",
    "    for thr in thresholds:\n",
    "        report = prediction_report(model, train_seq, patch_size=patch_size, stride=patch_size, threshold=thr)\n",
    "        print(f\"{thr=}\")\n",
    "        pd.options.display.float_format = \"{:,.2f}\".format\n",
    "        df = pd.DataFrame(report)\n",
    "        print(df, \"\\n\")\n",
    "\n",
    "metric = 'accuracy'\n",
    "# best_thr = find_best_threshold(model, train_gen, patch_size, patch_size, metric, 10)\n",
    "linear_search(model, val_gen, patch_size, patch_size, 20)\n",
    "print(f\"Best threshold={best_thr[1]} gives {metric}={best_thr[0]} on training set (without augmentation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# model.load_weights('augmented_adam_dice_best-Copy1.h5')\n",
    "model.load_weights(best_checkpoint)\n",
    "# report = prediction_report(model, test_gen, patch_size=patch_size, stride=patch_size, threshold=best_thr[1])\n",
    "report = prediction_report(\n",
    "    model, test_gen, patch_size=patch_size, stride=patch_size, threshold=0.334\n",
    ")\n",
    "\n",
    "# pprint.pprint(report)\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "df = pd.DataFrame(report)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# model.load_weights('augmented_adam_dice_best-Copy1.h5')\n",
    "# model.load_weights(best_checkpoint)\n",
    "# report = prediction_report(model, test_gen, patch_size=patch_size, stride=patch_size, threshold=best_thr[1])\n",
    "report = prediction_report(\n",
    "    model, test_gen, patch_size=patch_size, stride=patch_size, threshold=0.5\n",
    ")\n",
    "\n",
    "# pprint.pprint(report)\n",
    "\n",
    "pd.options.display.float_format = \"{:,.2f}\".format\n",
    "df = pd.DataFrame(report)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using fov\n",
    "\n",
    "sgd bce unet\n",
    "thr=0.25\n",
    "support     42,236,884.00 4,216,591.00\n",
    "accuracy             0.96         0.96\n",
    "precision            0.98         0.78\n",
    "recall               0.98         0.83\n",
    "f1                   0.98         0.81\n",
    "sensitivity          0.98         0.83\n",
    "specifity            0.83         0.98 \n",
    "\n",
    "thr=0.35\n",
    "                        0            1\n",
    "support     42,236,884.00 4,216,591.00\n",
    "accuracy             0.97         0.97\n",
    "precision            0.98         0.83\n",
    "recall               0.98         0.78\n",
    "f1                   0.98         0.80\n",
    "sensitivity          0.98         0.78\n",
    "specifity            0.78         0.98 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Adam(1e-4) TverskyLoss(a=0.3, b=0.7) resnet34 patch_size=1024  batch_size=2 200 epochs\n",
    "resnet_34_augmented_adam1e4_tversky_*-Copy1\n",
    "                        0            1\n",
    "support     37,538,905.00 3,943,117.00\n",
    "accuracy             0.96         0.96\n",
    "precision            0.99         0.72\n",
    "recall               0.96         0.87\n",
    "f1                   0.98         0.79\n",
    "sensitivity          0.96         0.87\n",
    "specifity            0.87         0.96\n",
    "\n",
    "Adam(4e-5) TverskyLoss(a=0.3, b=0.7) resnet34 patch_size=1024  batch_size=2 200 epochs\n",
    "resnet_34_augmented_adam_tversky_*-Copy1   ()\n",
    "                        0            1\n",
    "support     37,538,905.00 3,943,117.00\n",
    "accuracy             0.96         0.96\n",
    "precision            0.99         0.73\n",
    "recall               0.97         0.86\n",
    "f1                   0.98         0.79\n",
    "sensitivity          0.97         0.86\n",
    "specifity            0.86         0.97\n",
    "\n",
    "Adam(1e-5) DiceLoss([0.1, 1]) resnet34 patch_size=512  batch_size=1 500 epochs\n",
    "resnet_34_augmented_adam_dice_class_weights\n",
    "                        0            1\n",
    "support     37,538,905.00 3,943,117.00\n",
    "accuracy             0.95         0.95\n",
    "precision            0.97         0.78\n",
    "recall               0.98         0.73\n",
    "f1                   0.98         0.75\n",
    "sensitivity          0.98         0.73\n",
    "specifity            0.73         0.98\n",
    "\n",
    "Adam(4e-5) DiceLoss() resnet34 patch_size=1024 batch_size=2 epochs=250\n",
    "                        0            1\n",
    "support     37,538,905.00 3,943,117.00\n",
    "accuracy             0.96         0.96\n",
    "precision            0.98         0.81\n",
    "recall               0.98         0.78\n",
    "f1                   0.98         0.80\n",
    "sensitivity          0.98         0.78\n",
    "specifity            0.78         0.98\n",
    "\n",
    "----------------------\n",
    "\n",
    "Adam(8e-5) DiceLoss()+JaccardLoss() resnet34 patch_size=512 batch_size=8 epochs=300\n",
    "                        0            1\n",
    "support     30,418,540.00 2,322,836.00\n",
    "accuracy             0.97         0.97\n",
    "precision            0.98         0.82\n",
    "recall               0.99         0.69\n",
    "f1                   0.98         0.75\n",
    "sensitivity          0.99         0.69\n",
    "specifity            0.69         0.99\n",
    "\n",
    "Adam(1e-4) DiceLoss() resnet34 patch_size=1024 batch_size=2 epochs=500\n",
    "                        0            1\n",
    "support     53,264,590.00 4,032,818.00\n",
    "accuracy             0.97         0.97\n",
    "precision            0.98         0.79\n",
    "recall               0.98         0.80\n",
    "f1                   0.98         0.79\n",
    "sensitivity          0.98         0.80\n",
    "specifity            0.80         0.98\n",
    "\n",
    "Adam(1e-5) DiceLoss() resnet34 patch_size=1024 batch_size=2 epochs=500\n",
    "                        0            1\n",
    "support     53,264,590.00 4,032,818.00\n",
    "accuracy             0.97         0.97\n",
    "precision            0.99         0.78\n",
    "recall               0.98         0.81\n",
    "f1                   0.98         0.79\n",
    "sensitivity          0.98         0.81\n",
    "specifity            0.81         0.98\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
